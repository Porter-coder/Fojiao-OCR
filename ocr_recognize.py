# ocr_recognize.py â€” ç”¨ EasyOCRï¼ˆGPUåŠ é€Ÿç‰ˆæœ¬ï¼‰
import os
import json
import uuid
import shutil
import csv
import re
import logging
import platform
import warnings
from concurrent.futures import ThreadPoolExecutor, as_completed
from tqdm import tqdm
from PIL import Image, ImageDraw, ImageFont

# åŠ è½½ç¯å¢ƒå˜é‡
try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    pass  # å¦‚æœæ²¡æœ‰å®‰è£… python-dotenvï¼Œç»§ç»­æ‰§è¡Œ

# ç¦ç”¨PyTorch RNNè­¦å‘Š
warnings.filterwarnings('ignore', category=UserWarning, module='torch.nn.modules.rnn')

# -------------------- DeepSeek APIé…ç½® --------------------
try:
    from openai import OpenAI, APITimeoutError, APIError
    DEEPSEEK_API_KEY = os.getenv("DEEPSEEK_API_KEY", "sk-fbab52c876d64fa2b9a22fd47b4aa6d1")
    DEEPSEEK_BASE_URL = os.getenv("DEEPSEEK_BASE_URL", "https://api.deepseek.com/v1")
    DEEPMODEL = os.getenv("DEEPMODEL", "deepseek-chat")

    deepseek_client = OpenAI(
        api_key=DEEPSEEK_API_KEY,
        base_url=DEEPSEEK_BASE_URL,
        timeout=30,
        max_retries=1
    )
    print("âœ“ DeepSeek APIåˆå§‹åŒ–æˆåŠŸ")
    deepseek_available = True
except ImportError:
    print("âš  æœªå®‰è£… openaiï¼ŒAIè§£æåŠŸèƒ½å°†ä¸å¯ç”¨")
    deepseek_available = False
except Exception as e:
    print(f"âš  DeepSeek APIåˆå§‹åŒ–å¤±è´¥: {e}")
    deepseek_available = False

# -------------------- AIè§£æå‡½æ•° --------------------
def clean_json_output(text: str) -> str:
    """æ¸…ç†AIè¿”å›çš„JSONå­—ç¬¦ä¸²"""
    text = text.strip()
    text = re.sub(r"^```(?:json)?\s*", "", text, flags=re.IGNORECASE)
    text = re.sub(r"\s*```$", "", text)
    return text

def build_ai_parse_prompt(ocr_text: str, question_num: int) -> list:
    """æ„å»ºAIè§£ææç¤ºè¯ - ä»¿ç…§deepseek_process.py"""
    system = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„é¢˜ç›®æ ¡å¯¹ä¸ç­”é¢˜åŠ©æ‰‹ã€‚å­¦ç§‘: é€šç”¨é¢˜åº“ã€‚
è¯·ä¸¥æ ¼æŒ‰è¦æ±‚å¤„ç†é¢˜ç›®å¹¶è¿”å› JSONã€‚"""

    user = f"""è¯·å¯¹ä»¥ä¸‹ OCR è¯†åˆ«çš„é¢˜ç›®è¿›è¡Œå¤„ç†ï¼š

1. çº æ­£é¢˜å¹²å’Œé€‰é¡¹ä¸­çš„é”™åˆ«å­—ã€ä¹±ç ã€ç—…å¥
2. åˆ†æé¢˜ç›®ç±»å‹ï¼ˆå•é€‰/å¤šé€‰/åˆ¤æ–­ï¼‰
3. ç»™å‡ºæ­£ç¡®ç­”æ¡ˆå’Œè¯¦ç»†è§£æ
4. æä¾›ä¿®å¤ç†ç”±å’Œåˆ¤æ–­ç†ç”±
5. ä¸¥æ ¼è¿”å› JSONï¼Œä¸è¦å…¶ä»–æ–‡å­—

åŸå§‹ OCR å†…å®¹ï¼š
{ocr_text}

è¿”å› JSON æ ¼å¼ï¼š
{{
    "é¢˜å·": {question_num},
    "ç±»å‹": "å•é€‰/å¤šé€‰/åˆ¤æ–­",
    "åŸå§‹é¢˜ç›®": "OCRåŸæ–‡",
    "é¢˜ç›®": "çº æ­£åçš„é¢˜ç›®",
    "é€‰é¡¹": {{"A": "...", "B": "...", "C": "...", "D": "..."}},
    "é¢˜ç›®å·²ä¿®æ­£": true/false,
    "ä¿®æ­£è¯´æ˜": "ä¿®æ­£äº†ä»€ä¹ˆå†…å®¹ï¼ˆæ²¡ä¿®æ­£åˆ™ä¸ºç©ºï¼‰",
    "ç­”æ¡ˆ": "A/B/C/D æˆ– å¯¹/é”™",(å¦‚æœæ˜¯å¤šé€‰ï¼Œè¿”å›ABï¼ŒABCï¼ŒBCDç±»ä¼¼æ ¼å¼ï¼Œä¸è¦åŠ åˆ†éš”ç¬¦)
    "è§£æ": "ç­”æ¡ˆè§£æ",
    "ä¿®å¤ç†ç”±": "ä¸ºä»€ä¹ˆéœ€è¦ä¿®å¤OCRè¯†åˆ«ç»“æœ",
    "åˆ¤æ–­ç†ç”±": "ä¸ºä»€ä¹ˆè¿™æ ·åˆ¤æ–­é¢˜å‹å’Œç­”æ¡ˆ"
}}

æ³¨æ„ï¼š
- åˆ¤æ–­é¢˜ä¸éœ€è¦é€‰é¡¹å­—æ®µ
- ä¿®å¤ç†ç”±ï¼šè¯¦ç»†è¯´æ˜OCRè¯†åˆ«çš„é”™è¯¯å’Œä¿®æ­£ä¾æ®
- åˆ¤æ–­ç†ç”±ï¼šè¯´æ˜é¢˜å‹åˆ¤æ–­å’Œç­”æ¡ˆæ¨æ–­çš„é€»è¾‘"""

    return [
        {"role": "system", "content": system},
        {"role": "user", "content": user}
    ]

def parse_question_with_ai(ocr_text: str, question_num: int) -> dict:
    """ä½¿ç”¨AIè§£æé¢˜ç›®ç»“æ„ - ä»¿ç…§deepseek_process.py"""
    if not deepseek_available:
        raise RuntimeError("DeepSeek APIä¸å¯ç”¨ï¼Œè¯·å®‰è£…openaiåº“")

    messages = build_ai_parse_prompt(ocr_text, question_num)

    try:
        response = deepseek_client.chat.completions.create(
            model=DEEPMODEL,
            messages=messages,
            temperature=0
        )

        raw = response.choices[0].message.content
        if not raw:
            raise RuntimeError("AIè¿”å›å†…å®¹ä¸ºç©º")

        cleaned = clean_json_output(raw)
        parsed = json.loads(cleaned)

        # éªŒè¯å¿…éœ€å­—æ®µ - ä»¿ç…§deepseek_process.py
        required_fields = ["é¢˜å·", "ç±»å‹", "é¢˜ç›®", "é€‰é¡¹", "ç­”æ¡ˆ", "è§£æ"]
        for field in required_fields:
            if field not in parsed:
                raise RuntimeError(f"AIè¿”å›ç¼ºå°‘å¿…éœ€å­—æ®µ: {field}")

        # è½¬æ¢é€‰é¡¹æ ¼å¼ - ä»{"A": "...", "B": "..."}è½¬æ¢ä¸º{"A": "...", "B": "...", ...}
        options = parsed.get("é€‰é¡¹", {})
        if isinstance(options, dict):
            parsed["é€‰é¡¹A"] = options.get("A", "")
            parsed["é€‰é¡¹B"] = options.get("B", "")
            parsed["é€‰é¡¹C"] = options.get("C", "")
            parsed["é€‰é¡¹D"] = options.get("D", "")
            parsed["é€‰é¡¹E"] = options.get("E", "")

        return parsed

    except APITimeoutError:
        raise RuntimeError("AIè§£æè¶…æ—¶")
    except APIError as e:
        raise RuntimeError(f"AI APIé”™è¯¯: {str(e)}")
    except json.JSONDecodeError as e:
        raise RuntimeError(f"AIè¿”å›JSONæ ¼å¼é”™è¯¯: {str(e)}")
    except Exception as e:
        raise RuntimeError(f"AIè§£æå¤±è´¥: {str(e)}")

# GPUåŠ é€Ÿç›¸å…³ç¯å¢ƒå˜é‡è®¾ç½® - åœ¨å¯¼å…¥å‰è®¾ç½®
os.environ['OMP_NUM_THREADS'] = '1'  # é¿å…çº¿ç¨‹å†²çª
os.environ['MKL_NUM_THREADS'] = '1'

try:
    import easyocr
    print("âœ“ ä½¿ç”¨EasyOCR (æ”¯æŒGPUåŠ é€Ÿ)")
except ImportError:
    print("é”™è¯¯ï¼šæœªå®‰è£… easyocr")
    print("è¯·è¿è¡Œ: pip install easyocr")
    exit(1)

# åˆ›å»ºä»»åŠ¡IDå’Œç›®å½•ç»“æ„
import time
TIMESTAMP = time.strftime("%Y%m%d_%H%M%S")  # æ—¶é—´æˆ³åˆ°ç§’
TASK_ID = str(uuid.uuid4())[:8]  # ä½¿ç”¨8ä½UUIDä½œä¸ºä»»åŠ¡ID
SCREENSHOT_DIR = "screenshots"
SCREENSHOT_TEMP_DIR = f"screenshot_temp/{TIMESTAMP}_{TASK_ID}"
PROCESSING_DIR = f"processing/{TIMESTAMP}/{TASK_ID}/ocr"  # ä¸­é—´ç»“æœç›®å½•
OUTPUT_DIR = f"output/{TIMESTAMP}/{TASK_ID}"  # æœ€ç»ˆç»“æœç›®å½•
OCR_THREADS = 12

# åˆ›å»ºä»»åŠ¡ä¸“ç”¨çš„ç›®å½•ç»“æ„
os.makedirs(SCREENSHOT_TEMP_DIR, exist_ok=True)
os.makedirs(f"{PROCESSING_DIR}/images", exist_ok=True)  # ä¸­é—´ç»“æœï¼šæ ‡æ³¨å›¾ç‰‡
os.makedirs(f"{PROCESSING_DIR}/texts", exist_ok=True)   # ä¸­é—´ç»“æœï¼šæ–‡æœ¬æ–‡ä»¶
os.makedirs(f"{PROCESSING_DIR}/details", exist_ok=True) # ä¸­é—´ç»“æœï¼šè¯¦ç»†JSON
os.makedirs(OUTPUT_DIR, exist_ok=True)  # æœ€ç»ˆç»“æœç›®å½•

# ç§»åŠ¨æˆªå›¾æ–‡ä»¶åˆ°ä¸´æ—¶ç›®å½•
if os.path.exists(SCREENSHOT_DIR) and os.listdir(SCREENSHOT_DIR):
    print(f"ç§»åŠ¨æˆªå›¾æ–‡ä»¶åˆ°ä¸´æ—¶ç›®å½•: {SCREENSHOT_TEMP_DIR}")
    for filename in os.listdir(SCREENSHOT_DIR):
        src_path = os.path.join(SCREENSHOT_DIR, filename)
        dst_path = os.path.join(SCREENSHOT_TEMP_DIR, filename)
        if os.path.isfile(src_path):
            shutil.move(src_path, dst_path)
    print(f"âœ“ å·²ç§»åŠ¨ {len(os.listdir(SCREENSHOT_TEMP_DIR))} ä¸ªæ–‡ä»¶")
else:
    print(f"âš  æºç›®å½• {SCREENSHOT_DIR} ä¸å­˜åœ¨æˆ–ä¸ºç©º")

logging.basicConfig(level=logging.INFO, format="%(asctime)s [%(levelname)s] %(message)s")
logger = logging.getLogger("ocr")

def flatten_rnn_parameters(model):
    """é€’å½’åœ°å‹ç¼©RNNæ¨¡å‹çš„æƒé‡åˆ°è¿ç»­å†…å­˜å—"""
    try:
        import torch
        for module in model.modules():
            if isinstance(module, (torch.nn.RNNBase, torch.nn.LSTM, torch.nn.GRU)):
                try:
                    module.flatten_parameters()
                except RuntimeError:
                    # å¦‚æœå·²ç»åœ¨è¿ç»­å†…å­˜ä¸­ï¼Œflatten_parameters()ä¼šæŠ›å‡ºå¼‚å¸¸ï¼Œå¿½ç•¥å³å¯
                    pass
    except ImportError:
        # å¦‚æœæ²¡æœ‰torchï¼Œè·³è¿‡è¿™ä¸ªä¼˜åŒ–
        pass

def init_ocr_with_gpu():
    """åˆå§‹åŒ–EasyOCR"""
    try:
        print("æ­£åœ¨åˆå§‹åŒ– EasyOCR...")

        # æ£€æŸ¥GPUå¯ç”¨æ€§
        gpu_available = check_gpu_availability()

        if gpu_available:
            print("âœ“ æ£€æµ‹åˆ°GPUï¼ŒEasyOCRå°†ä½¿ç”¨GPUåŠ é€Ÿ")
            # EasyOCRè‡ªåŠ¨æ£€æµ‹GPU
            reader = easyocr.Reader(['ch_sim', 'en'], gpu=True)
        else:
            print("âš  æœªæ£€æµ‹åˆ°GPUï¼ŒEasyOCRå°†ä½¿ç”¨CPUæ¨¡å¼")
            reader = easyocr.Reader(['ch_sim', 'en'], gpu=False)

        # ä¿®å¤RNNæƒé‡å†…å­˜è¿ç»­æ€§è­¦å‘Š
        try:
            import torch
            if hasattr(reader, 'recognition_network'):
                flatten_rnn_parameters(reader.recognition_network)
                print("âœ“ RNNæƒé‡å·²å‹ç¼©åˆ°è¿ç»­å†…å­˜")
            elif hasattr(reader, 'recognizer'):
                flatten_rnn_parameters(reader.recognizer)
                print("âœ“ RNNæƒé‡å·²å‹ç¼©åˆ°è¿ç»­å†…å­˜")
        except Exception as e:
            # å¦‚æœæ— æ³•è®¿é—®å†…éƒ¨æ¨¡å‹ï¼Œå¿½ç•¥è¿™ä¸ªä¼˜åŒ–
            pass

        print("âœ“ EasyOCRåŠ è½½å®Œæˆï¼")
        return reader, gpu_available

    except Exception as e:
        logger.error(f"EasyOCRåˆå§‹åŒ–å¤±è´¥: {e}")
        raise e

def check_gpu_availability():
    """æ£€æŸ¥GPUæ˜¯å¦å¯ç”¨"""
    try:
        import torch
        gpu_available = torch.cuda.is_available()
        if gpu_available:
            gpu_count = torch.cuda.device_count()
            gpu_name = torch.cuda.get_device_name(0)
            print(f"âœ“ PyTorchæ£€æµ‹åˆ°GPU: {gpu_name} ({gpu_count}ä¸ª)")
            return True
        else:
            print("âš  PyTorchæœªæ£€æµ‹åˆ°GPU")
            return False
    except ImportError:
        print("âš  æœªå®‰è£…PyTorchï¼Œä½¿ç”¨CPUæ¨¡å¼")
        return False
    except Exception as e:
        print(f"GPUæ£€æµ‹å¤±è´¥: {e}")
        return False

def optimize_thread_count(use_gpu):
    """æ ¹æ®æ˜¯å¦ä½¿ç”¨GPUä¼˜åŒ–çº¿ç¨‹æ•°"""
    if use_gpu:
        # GPUåŠ é€Ÿæ—¶å‡å°‘çº¿ç¨‹æ•°ï¼Œé¿å…GPUå†…å­˜ä¸è¶³
        return min(4, os.cpu_count() or 4)
    else:
        # CPUæ¨¡å¼ä½¿ç”¨æ›´å¤šçº¿ç¨‹
        return min(12, os.cpu_count() or 8)

# æ£€æŸ¥GPUå¯ç”¨æ€§
gpu_available = check_gpu_availability()

print(f"ä»»åŠ¡ID: {TASK_ID}")
print("æ­£åœ¨åˆå§‹åŒ– RapidOCR...")
ocr, use_gpu = init_ocr_with_gpu()
print(f"ä½¿ç”¨æ¨¡å¼: {'GPUåŠ é€Ÿ' if use_gpu else 'CPU'}")

# æ ¹æ®GPUä½¿ç”¨æƒ…å†µä¼˜åŒ–çº¿ç¨‹æ•°
OCR_THREADS = optimize_thread_count(use_gpu)
print(f"ä¼˜åŒ–çº¿ç¨‹æ•°: {OCR_THREADS}\n")

def parse_question_type(lines):
    for line in lines:
        if "å•é€‰" in line: return "å•é€‰"
        if "å¤šé€‰" in line: return "å¤šé€‰"
        if "åˆ¤æ–­" in line: return "åˆ¤æ–­"
    return "æœªçŸ¥"

def parse_options(lines):
    """æ”¹è¿›çš„é€‰é¡¹è§£æï¼Œæ”¯æŒè·¨è¡Œé€‰é¡¹"""
    opt_map = {"A": "", "B": "", "C": "", "D": "", "E": ""}

    i = 0
    while i < len(lines):
        line = lines[i].strip()
        if not line:
            i += 1
            continue

        # æ£€æŸ¥æ˜¯å¦æ˜¯é€‰é¡¹å¼€å§‹è¡Œ (A. B. C. D. E.)
        match = re.match(r'^([A-E])[.ï¼ã€:\s]*(.*)', line)
        if match:
            option_key = match.group(1)
            option_content = match.group(2).strip()

            # å¦‚æœé€‰é¡¹å†…å®¹ä¸ºç©ºï¼Œå°è¯•åˆå¹¶ä¸‹ä¸€è¡Œ
            if not option_content and i + 1 < len(lines):
                next_line = lines[i + 1].strip()
                # å¦‚æœä¸‹ä¸€è¡Œä¸ä»¥é€‰é¡¹å­—æ¯å¼€å¤´ï¼Œå°±åˆå¹¶
                if next_line and not re.match(r'^[A-E][.ï¼ã€:\s]', next_line):
                    option_content = next_line
                    i += 1  # è·³è¿‡ä¸‹ä¸€è¡Œ

            opt_map[option_key] = f"{option_key}. {option_content}" if option_content else f"{option_key}."
        i += 1

    return opt_map

def save_results_to_csv(results, output_path):
    """å°†OCRç»“æœä¿å­˜ä¸ºCSVæ ¼å¼ - åºå·+é¢˜å‹+é¢˜å¹²+é€‰é¡¹A+é€‰é¡¹B+é€‰é¡¹C+é€‰é¡¹D+é€‰é¡¹E+ç­”æ¡ˆ+è§£æ"""
    if not results:
        return

    # CSVå¤´éƒ¨ - ä»¿ç…§deepseek_process.py
    headers = ['åºå·', 'é¢˜å‹', 'é¢˜å¹²', 'é€‰é¡¹A', 'é€‰é¡¹B', 'é€‰é¡¹C', 'é€‰é¡¹D', 'é€‰é¡¹E', 'ç­”æ¡ˆ', 'è§£æ', 'å·²ä¿®æ­£', 'ä¿®æ­£è¯´æ˜']

    with open(output_path, 'w', newline='', encoding='utf-8-sig') as csvfile:
        writer = csv.DictWriter(csvfile, fieldnames=headers)
        writer.writeheader()

        for result in results:
            # ç›´æ¥ä½¿ç”¨AIè§£æçš„ç»“æœ
            options = result.get('é€‰é¡¹', {})

            row = {
                'åºå·': result.get('åºå·', ''),
                'é¢˜å‹': result.get('ç±»å‹', ''),
                'é¢˜å¹²': result.get('é¢˜å¹²', ''),
                'é€‰é¡¹A': options.get('A', ''),
                'é€‰é¡¹B': options.get('B', ''),
                'é€‰é¡¹C': options.get('C', ''),
                'é€‰é¡¹D': options.get('D', ''),
                'é€‰é¡¹E': options.get('E', ''),
                'ç­”æ¡ˆ': result.get('ç­”æ¡ˆ', ''),
                'è§£æ': result.get('è§£æ', '').replace('\n', ' '),
                'å·²ä¿®æ­£': 'æ˜¯' if result.get('é¢˜ç›®å·²ä¿®æ­£') else 'å¦',
                'ä¿®æ­£è¯´æ˜': result.get('ä¿®æ­£è¯´æ˜', '')
            }
            writer.writerow(row)

def filter_answer_context(details):
    """
    å®Œå…¨è¿‡æ»¤æ‰ç­”æ¡ˆç›¸å…³çš„å†…å®¹åŠå…¶ä¸Šä¸‹æ–‡
    åŒ…æ‹¬ï¼šæˆ‘çš„ç­”æ¡ˆã€å‚è€ƒç­”æ¡ˆã€ç­”æ¡ˆè§£æåŠå…¶åé¢çš„æ‰€æœ‰ç›¸å…³å†…å®¹
    """
    filtered = []
    skip_mode = False  # æ˜¯å¦å¤„äºè·³è¿‡æ¨¡å¼
    skip_keywords = ["æˆ‘çš„ç­”æ¡ˆ", "å‚è€ƒç­”æ¡ˆ", "ç­”æ¡ˆè§£æ"]

    for i, line in enumerate(details):
        text = line["text"].strip()

        # æ£€æŸ¥æ˜¯å¦é‡åˆ°éœ€è¦è·³è¿‡çš„å…³é”®è¯
        found_skip_keyword = False
        for keyword in skip_keywords:
            if keyword in text:
                found_skip_keyword = True
                skip_mode = True
                break

        # å¦‚æœæ‰¾åˆ°è·³è¿‡å…³é”®è¯ï¼Œå¼€å§‹è·³è¿‡æ¨¡å¼
        if found_skip_keyword:
            continue

        # å¦‚æœå¤„äºè·³è¿‡æ¨¡å¼ï¼Œç»§ç»­è·³è¿‡
        if skip_mode:
            continue

        # æ­£å¸¸å†…å®¹ï¼Œæ·»åŠ åˆ°è¿‡æ»¤ç»“æœä¸­
        line_copy = line.copy()
        line_copy["index"] = len(filtered) + 1
        filtered.append(line_copy)

    return filtered

def save_annotated_image(image_path, boxes, output_path):
    try:
        img = Image.open(image_path)
        draw = ImageDraw.Draw(img)
        try:
            font = ImageFont.truetype("msyh.ttc", 14)
        except:
            font = ImageFont.load_default()
        
        for i, item in enumerate(boxes):
            box = item['poly']
            conf = item['confidence']
            points = [(int(p[0]), int(p[1])) for p in box]
            points.append(points[0])
            draw.line(points, fill="red", width=2)
            draw.text((int(box[0][0]), int(box[0][1])-18), f"{i+1}|{conf:.2f}", fill="blue", font=font)
        
        img.save(output_path)
    except Exception as e:
        logger.warning(f"ä¿å­˜æ ‡æ³¨å›¾ç‰‡å¤±è´¥: {e}")

def process_image(item):
    idx, filename = item
    image_path = os.path.join(SCREENSHOT_TEMP_DIR, filename)
    base_name = os.path.splitext(filename)[0]

    try:
        # EasyOCR API
        import time
        start_time = time.time()

        result = ocr.readtext(image_path)
        elapse = time.time() - start_time
    except Exception as e:
        logger.error(f"OCR å¤±è´¥ {filename}: {e}")
        return {"åºå·": idx + 1, "æ–‡ä»¶": filename, "é”™è¯¯": str(e)}
    
    if not result:
        return {"åºå·": idx + 1, "æ–‡ä»¶": filename, "é”™è¯¯": "OCR è¿”å›ä¸ºç©º"}
    
    # å¤„ç†EasyOCRç»“æœ
    raw_details = []
    try:
        # EasyOCRè¿”å›æ ¼å¼: [[bbox, text, confidence], ...]
        for i, (bbox, text, confidence) in enumerate(result):
            raw_details.append({
                "index": i + 1,
                "text": text,
                "confidence": round(float(confidence), 4),
                "poly": [[int(p[0]), int(p[1])] for p in bbox]
            })

    except Exception as e:
        print(f"ç»“æœè§£æé”™è¯¯: {e}")
        print(f"ç»“æœç±»å‹: {type(result)}")
        if result:
            print(f"ç¬¬ä¸€ä¸ªç»“æœ: {result[0] if len(result) > 0 else 'ç©º'}")

    # è¿‡æ»¤ç­”æ¡ˆç›¸å…³å†…å®¹
    details = filter_answer_context(raw_details)

    # ä»è¿‡æ»¤åçš„detailsç”Ÿæˆtexts
    texts = [line["text"] for line in details]

    full_text = "\n".join(texts)
    lines = [t.strip() for t in texts if t.strip()]
    
    # ä¿å­˜ä¸­é—´ç»“æœåˆ°processingç›®å½•
    with open(f"{PROCESSING_DIR}/texts/{base_name}.txt", "w", encoding="utf-8") as f:
        for i, t in enumerate(texts):
            f.write(f"[{i+1}] {t}\n")

    with open(f"{PROCESSING_DIR}/details/{base_name}.json", "w", encoding="utf-8") as f:
        json.dump({"file": filename, "total_lines": len(texts), "lines": details}, f, ensure_ascii=False, indent=2)

    save_annotated_image(image_path, details, f"{PROCESSING_DIR}/images/{base_name}_ocr.png")

    # åªè¿”å›OCRç»“æœï¼Œåç»­ç»Ÿä¸€è¿›è¡ŒAIè§£æ
    result = {
        "åºå·": idx + 1,
        "æ–‡ä»¶": filename,
        "åŸå§‹æ–‡æœ¬": full_text,
        "è¡Œæ•°": len(texts),
        "è§£æçŠ¶æ€": "å¾…AIè§£æ"
    }

    # è°ƒè¯•ä¿¡æ¯
    if idx < 3:  # åªæ‰“å°å‰3ä¸ªç»“æœçš„è°ƒè¯•ä¿¡æ¯
        print(f"OCRå®Œæˆ {filename}: {len(texts)} è¡Œæ–‡æœ¬")
    return result

def cleanup_empty_temp_dirs():
    """æ¸…ç†ç©ºçš„tempç›®å½•"""
    if not os.path.exists("screenshot_temp"):
        return

    cleaned_count = 0
    for dir_name in os.listdir("screenshot_temp"):
        temp_path = os.path.join("screenshot_temp", dir_name)
        if os.path.isdir(temp_path):
            # æ£€æŸ¥ç›®å½•æ˜¯å¦ä¸ºç©ºæˆ–åªåŒ…å«éå›¾ç‰‡æ–‡ä»¶
            try:
                files = os.listdir(temp_path)
                image_count = len([f for f in files
                                  if f.lower().endswith(('.png', '.jpg', '.jpeg'))])
                if image_count == 0:
                    # åˆ é™¤ç©ºç›®å½•
                    import shutil
                    shutil.rmtree(temp_path)
                    cleaned_count += 1
                    print(f"å·²æ¸…ç†ç©ºtempç›®å½•: {dir_name}")
            except Exception as e:
                print(f"æ¸…ç†ç›®å½• {dir_name} æ—¶å‡ºé”™: {e}")

    if cleaned_count > 0:
        print(f"å…±æ¸…ç†äº† {cleaned_count} ä¸ªç©ºtempç›®å½•")
    return cleaned_count

def select_data_source():
    """äº¤äº’å¼é€‰æ‹©æ•°æ®æº"""
    print("=== æ•°æ®æºé€‰æ‹© ===")

    # é¦–å…ˆæ¸…ç†ç©ºçš„tempç›®å½•
    cleanup_empty_temp_dirs()
    print()

    # æ£€æŸ¥ç°æœ‰çš„screenshot_tempç›®å½•
    temp_dirs = []
    if os.path.exists("screenshot_temp"):
        temp_dirs = [d for d in os.listdir("screenshot_temp")
                    if os.path.isdir(os.path.join("screenshot_temp", d))]

    # æ£€æŸ¥screenshotsç›®å½•
    has_new_screenshots = os.path.exists(SCREENSHOT_DIR) and \
                         any(f.lower().endswith(('.png', '.jpg', '.jpeg'))
                             for f in os.listdir(SCREENSHOT_DIR))

    print("å¯ç”¨æ•°æ®æº:")
    print("0. é‡æ–°æˆªå–æ–°çš„æˆªå›¾")

    for i, temp_dir in enumerate(temp_dirs, 1):
        temp_path = os.path.join("screenshot_temp", temp_dir)
        png_count = len([f for f in os.listdir(temp_path)
                        if f.lower().endswith(('.png', '.jpg', '.jpeg'))])
        print(f"{i}. ä½¿ç”¨ç°æœ‰æˆªå›¾: {temp_dir} ({png_count}å¼ å›¾ç‰‡)")

    if has_new_screenshots:
        new_count = len([f for f in os.listdir(SCREENSHOT_DIR)
                        if f.lower().endswith(('.png', '.jpg', '.jpeg'))])
        print(f"{len(temp_dirs)+1}. ä½¿ç”¨screenshotsç›®å½• ({new_count}å¼ å›¾ç‰‡)")

    while True:
        try:
            choice = input("\nè¯·é€‰æ‹©æ•°æ®æº (è¾“å…¥æ•°å­—): ").strip()

            if choice == "0":
                print("è¯·å…ˆæˆªå–æ–°çš„æˆªå›¾åˆ°screenshotsç›®å½•ï¼Œç„¶åé‡æ–°è¿è¡Œç¨‹åº")
                return None, None

            choice_num = int(choice)

            if 1 <= choice_num <= len(temp_dirs):
                selected_dir = os.path.join("screenshot_temp", temp_dirs[choice_num-1])
                print(f"âœ“ é€‰æ‹©ä½¿ç”¨: {temp_dirs[choice_num-1]}")
                return selected_dir, temp_dirs[choice_num-1]

            elif has_new_screenshots and choice_num == len(temp_dirs) + 1:
                selected_dir = SCREENSHOT_DIR
                print("âœ“ é€‰æ‹©ä½¿ç”¨screenshotsç›®å½•")
                return selected_dir, "new_screenshots"

            else:
                print("æ— æ•ˆé€‰æ‹©ï¼Œè¯·é‡æ–°è¾“å…¥")

        except ValueError:
            print("è¯·è¾“å…¥æœ‰æ•ˆæ•°å­—")

def select_parsing_mode():
    """
    äº¤äº’å¼é€‰æ‹©AIè§£ææ¨¡å¼
    è¿”å›ï¼š(parsing_mode, mode_name)
    parsing_mode: "full" (å®Œæ•´ç­”æ¡ˆ) æˆ– "structure" (ä»…é¢˜åº“ç»“æ„)
    """
    print("\n" + "="*50)
    print("ğŸ¤– é€‰æ‹©AIè§£ææ¨¡å¼")
    print("="*50)
    print("1. å®Œæ•´ç­”æ¡ˆæ¨¡å¼ï¼šAIè§£æå‡ºå®Œæ•´çš„ç­”æ¡ˆå’Œè¯¦ç»†è§£æ")
    print("2. é¢˜åº“ç»“æ„æ¨¡å¼ï¼šAIåªè§£æé¢˜åº“ç»“æ„ï¼Œä¸è§£æç­”æ¡ˆ")
    print("="*50)

    while True:
        try:
            choice = input("è¯·é€‰æ‹©æ¨¡å¼ (1-2), æˆ–æŒ‰å›è½¦ä½¿ç”¨å®Œæ•´ç­”æ¡ˆæ¨¡å¼: ").strip()

            if not choice:  # æŒ‰å›è½¦é»˜è®¤å®Œæ•´ç­”æ¡ˆæ¨¡å¼
                print("âœ“ å·²é€‰æ‹©ï¼šå®Œæ•´ç­”æ¡ˆæ¨¡å¼")
                return "full", "å®Œæ•´ç­”æ¡ˆæ¨¡å¼"

            choice_num = int(choice)
            if choice_num == 1:
                print("âœ“ å·²é€‰æ‹©ï¼šå®Œæ•´ç­”æ¡ˆæ¨¡å¼")
                return "full", "å®Œæ•´ç­”æ¡ˆæ¨¡å¼"
            elif choice_num == 2:
                print("âœ“ å·²é€‰æ‹©ï¼šé¢˜åº“ç»“æ„æ¨¡å¼")
                return "structure", "é¢˜åº“ç»“æ„æ¨¡å¼"
            else:
                print("âŒ æ— æ•ˆé€‰æ‹©ï¼Œè¯·è¾“å…¥1æˆ–2")
        except ValueError:
            print("âŒ è¯·è¾“å…¥æœ‰æ•ˆçš„æ•°å­—")
        except KeyboardInterrupt:
            print("\nâŒ æ“ä½œå·²å–æ¶ˆ")
            return None, None

def main():
    import time
    start_time = time.time()

    # äº¤äº’å¼é€‰æ‹©æ•°æ®æº
    selected_source, source_name = select_data_source()
    if selected_source is None:
        return

    # äº¤äº’å¼é€‰æ‹©AIè§£ææ¨¡å¼
    parsing_mode, mode_name = select_parsing_mode()
    if parsing_mode is None:
        return

    # è®¾ç½®æ•°æ®æº
    global SCREENSHOT_TEMP_DIR
    if source_name != "new_screenshots":
        # ä½¿ç”¨ç°æœ‰tempç›®å½•
        SCREENSHOT_TEMP_DIR = selected_source
    # å¦‚æœé€‰æ‹©new_screenshotsï¼Œä¿æŒåŸæœ‰é€»è¾‘ï¼ˆä¼šç§»åŠ¨æ–‡ä»¶ï¼‰

    if not os.path.isdir(SCREENSHOT_TEMP_DIR):
        print(f"é”™è¯¯ï¼šæ‰¾ä¸åˆ° {SCREENSHOT_TEMP_DIR} æ–‡ä»¶å¤¹")
        return

    files = sorted([f for f in os.listdir(SCREENSHOT_TEMP_DIR) if f.lower().endswith(('.png', '.jpg', '.jpeg'))])
    if not files:
        print(f"{SCREENSHOT_TEMP_DIR} ä¸­æ²¡æœ‰å›¾ç‰‡ï¼")
        return

    print(f"å…± {len(files)} å¼ å›¾ç‰‡ï¼Œå¼€å§‹ OCR è¯†åˆ«...")
    print(f"ä»»åŠ¡ID: {TASK_ID}")
    print(f"ä¸´æ—¶æˆªå›¾ç›®å½•: {SCREENSHOT_TEMP_DIR}")
    print(f"ä¸­é—´ç»“æœç›®å½•: processing/{TIMESTAMP}/{TASK_ID}/ocr/")
    print(f"æœ€ç»ˆç»“æœç›®å½•: output/{TIMESTAMP}/{TASK_ID}/")
    print(f"ç³»ç»Ÿä¿¡æ¯: {platform.system()} {platform.release()}")
    print(f"Pythonç‰ˆæœ¬: {platform.python_version()}")
    print(f"GPUåŠ é€Ÿ: {'å¯ç”¨' if use_gpu else 'ç¦ç”¨'}")
    print("-" * 60)
    
    results = []
    errors = []
    
    items = list(enumerate(files))
    with ThreadPoolExecutor(max_workers=OCR_THREADS) as executor:
        futures = {executor.submit(process_image, it): it for it in items}
        for fut in tqdm(as_completed(futures), total=len(futures), desc="OCR"):
            try:
                res = fut.result()
                if res is None:
                    print("è­¦å‘Šï¼šprocess_imageè¿”å›None")
                    continue
                if "é”™è¯¯" in res:
                    errors.append(res)
                    print(f"é”™è¯¯ç»“æœ: {res.get('æ–‡ä»¶', 'unknown')} - {res.get('é”™è¯¯', 'unknown')}")
                else:
                    results.append(res)
                    if len(results) <= 3:  # åªæ‰“å°å‰3ä¸ªæˆåŠŸç»“æœ
                        print(f"æˆåŠŸç»“æœ: {res.get('æ–‡ä»¶', 'unknown')} - {res.get('è¡Œæ•°', 0)} è¡Œ")
            except Exception as e:
                logger.exception(f"å¤„ç†å¼‚å¸¸: {e}")
                print(f"å¼‚å¸¸è¯¦æƒ…: {e}")
    
    results.sort(key=lambda x: x.get("åºå·", 0))

    # ========== AIå¹¶å‘è§£æ ==========
    if deepseek_available and results and parsing_mode:
        print(f"\nå¼€å§‹AIå¹¶å‘è§£æ {len(results)} é“é¢˜ç›®...")

        # æ£€æŸ¥ç¼“å­˜æ–‡ä»¶
        cache_file = f"{PROCESSING_DIR}/ai_cache.json"
        ai_cache = {}
        if os.path.exists(cache_file):
            try:
                with open(cache_file, 'r', encoding='utf-8') as f:
                    ai_cache = json.load(f)
                print(f"åŠ è½½AIç¼“å­˜: {len(ai_cache)} ä¸ªå·²è§£æé¢˜ç›®")
            except Exception as e:
                print(f"åŠ è½½ç¼“å­˜å¤±è´¥: {e}")

        def ai_parse_task(question_data):
            """AIè§£æå•ä¸ªé¢˜ç›® - æ ¹æ®æ¨¡å¼å†³å®šè§£æå†…å®¹"""
            seq_num = question_data["åºå·"]

            # æ£€æŸ¥ç¼“å­˜
            cache_key = f"q_{seq_num}_{parsing_mode}"  # ä¸åŒæ¨¡å¼ä½¿ç”¨ä¸åŒç¼“å­˜
            if cache_key in ai_cache:
                print(f"ä½¿ç”¨ç¼“å­˜: é¢˜ç›® {seq_num} ({parsing_mode}æ¨¡å¼)")
                cached_data = ai_cache[cache_key]
                question_data.update(cached_data)
                return question_data

            try:
                parsed = parse_question_with_ai(question_data["åŸå§‹æ–‡æœ¬"], question_data["åºå·"])

                # æ ¹æ®è§£ææ¨¡å¼å†³å®šè¦æ›´æ–°çš„å­—æ®µ
                update_data = {
                    "ç±»å‹": parsed.get("ç±»å‹", "æœªçŸ¥"),
                    "åŸå§‹é¢˜ç›®": parsed.get("åŸå§‹é¢˜ç›®", ""),
                    "é¢˜ç›®": parsed.get("é¢˜ç›®", ""),
                    "é¢˜å¹²": parsed.get("é¢˜ç›®", ""),  # é¢˜å¹²ä½¿ç”¨çº æ­£åçš„é¢˜ç›®
                    "é€‰é¡¹": {
                        "A": parsed.get("é€‰é¡¹A", ""),
                        "B": parsed.get("é€‰é¡¹B", ""),
                        "C": parsed.get("é€‰é¡¹C", ""),
                        "D": parsed.get("é€‰é¡¹D", ""),
                        "E": parsed.get("é€‰é¡¹E", "")
                    },
                    "é¢˜ç›®å·²ä¿®æ­£": parsed.get("é¢˜ç›®å·²ä¿®æ­£", False),
                    "ä¿®æ­£è¯´æ˜": parsed.get("ä¿®æ­£è¯´æ˜", ""),
                    "è§£æçŠ¶æ€": f"AIè§£ææˆåŠŸ ({mode_name})"
                }

                # æ ¹æ®æ¨¡å¼å†³å®šæ˜¯å¦åŒ…å«ç­”æ¡ˆç›¸å…³ä¿¡æ¯
                if parsing_mode == "full":
                    # å®Œæ•´ç­”æ¡ˆæ¨¡å¼ï¼šåŒ…å«æ‰€æœ‰è§£æå†…å®¹
                    update_data.update({
                        "ç­”æ¡ˆ": parsed.get("ç­”æ¡ˆ", ""),
                        "è§£æ": parsed.get("è§£æ", ""),
                        "ä¿®å¤ç†ç”±": parsed.get("ä¿®å¤ç†ç”±", ""),
                        "åˆ¤æ–­ç†ç”±": parsed.get("åˆ¤æ–­ç†ç”±", ""),
                    })
                elif parsing_mode == "structure":
                    # é¢˜åº“ç»“æ„æ¨¡å¼ï¼šåªè§£æç»“æ„ï¼Œä¸è§£æç­”æ¡ˆ
                    update_data.update({
                        "ç­”æ¡ˆ": "",  # ä¸è§£æç­”æ¡ˆ
                        "è§£æ": "",  # ä¸è§£æè¯¦ç»†è§£æ
                        "ä¿®å¤ç†ç”±": "",
                        "åˆ¤æ–­ç†ç”±": "",
                    })
                question_data.update(update_data)

                # ä¿å­˜åˆ°ç¼“å­˜
                ai_cache[cache_key] = update_data.copy()
                try:
                    os.makedirs(os.path.dirname(cache_file), exist_ok=True)
                    with open(cache_file, 'w', encoding='utf-8') as f:
                        json.dump(ai_cache, f, ensure_ascii=False, indent=2)
                except Exception as e:
                    print(f"ä¿å­˜ç¼“å­˜å¤±è´¥: {e}")

                return question_data
            except Exception as e:
                # AIè§£æå¤±è´¥ï¼Œå›é€€åˆ°æ­£åˆ™è§£æ
                lines = [line.strip() for line in question_data["åŸå§‹æ–‡æœ¬"].strip().split('\n') if line.strip()]
                update_data = {
                    "ç±»å‹": parse_question_type(lines),
                    "é€‰é¡¹": parse_options(lines),
                    "é¢˜å¹²": "",  # æ­£åˆ™è§£æä¸æå–é¢˜å¹²
                    "é¢˜ç›®å·²ä¿®æ­£": False,
                    "ä¿®æ­£è¯´æ˜": "",
                    "ç­”æ¡ˆ": "",
                    "è§£æ": "",
                    "ä¿®å¤ç†ç”±": "",
                    "åˆ¤æ–­ç†ç”±": "",
                    "è§£æçŠ¶æ€": f"AIè§£æå¤±è´¥ï¼Œå›é€€æ­£åˆ™: {str(e)}"
                }
                question_data.update(update_data)

                # ä¿å­˜åˆ°ç¼“å­˜ï¼ˆå³ä½¿æ˜¯å¤±è´¥çš„ç»“æœï¼‰
                ai_cache[cache_key] = update_data.copy()
                try:
                    os.makedirs(os.path.dirname(cache_file), exist_ok=True)
                    with open(cache_file, 'w', encoding='utf-8') as f:
                        json.dump(ai_cache, f, ensure_ascii=False, indent=2)
                except Exception as cache_e:
                    print(f"ä¿å­˜ç¼“å­˜å¤±è´¥: {cache_e}")

                return question_data

        # å¹¶å‘AIè§£æ
        AI_THREADS = 20  # DeepSeekæ”¯æŒ20å¹¶å‘
        with ThreadPoolExecutor(max_workers=AI_THREADS) as executor:
            futures = {executor.submit(ai_parse_task, q): q for q in results}

            with tqdm(total=len(futures), desc="AIè§£æ", initial=0) as pbar:
                for fut in as_completed(futures):
                    try:
                        updated_question = fut.result()
                        # æ›´æ–°åŸresultsä¸­çš„å¯¹åº”é¡¹
                        for i, q in enumerate(results):
                            if q["åºå·"] == updated_question["åºå·"]:
                                results[i] = updated_question
                                break
                    except Exception as e:
                        logger.error(f"AIè§£æå¼‚å¸¸: {e}")
                    finally:
                        pbar.update(1)

        print(f"AIè§£æå®Œæˆï¼({mode_name})")

    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    os.makedirs(OUTPUT_DIR, exist_ok=True)

    # ä¿å­˜æœ€ç»ˆç»“æœåˆ°outputç›®å½•
    # æœºå™¨å¯è¯»æ ¼å¼
    with open(f"{OUTPUT_DIR}/ocr_results.json", "w", encoding="utf-8") as f:
        json.dump(results, f, ensure_ascii=False, indent=2)

    save_results_to_csv(results, f"{OUTPUT_DIR}/ocr_results.csv")

    # ä¿å­˜åˆ¤æ–­ç†ç”±å•ç‹¬çš„JSON
    reasoning_data = []
    for r in results:
        reasoning_data.append({
            "é¢˜å·": r.get("åºå·"),
            "åˆ¤æ–­ç†ç”±": r.get("åˆ¤æ–­ç†ç”±", ""),
            "ä¿®å¤ç†ç”±": r.get("ä¿®å¤ç†ç”±", "")
        })

    with open(f"{OUTPUT_DIR}/reasoning.json", "w", encoding="utf-8") as f:
        json.dump(reasoning_data, f, ensure_ascii=False, indent=2)

    # äººç±»å‹å¥½çš„æ ¼å¼
    with open(f"{OUTPUT_DIR}/ocr_summary.txt", "w", encoding="utf-8") as f:
        f.write("OCRè¯†åˆ«ç»“æœæ±‡æ€»\n")
        f.write("=" * 50 + "\n\n")
        f.write(f"ä»»åŠ¡ID: {TASK_ID}\n")
        f.write(f"å¤„ç†æ—¶é—´: {TIMESTAMP}\n")
        f.write(f"æ€»æ–‡ä»¶æ•°: {len(results)}\n")
        f.write(f"æˆåŠŸè¯†åˆ«: {len(results)} ä¸ªæ–‡ä»¶\n\n")

        for r in results:
            f.write(f"[{r['åºå·']}] {r['æ–‡ä»¶']} ({r['ç±»å‹']})\n")
            f.write(f"è¯†åˆ«è¡Œæ•°: {r['è¡Œæ•°']}\n")
            if r.get('é€‰é¡¹'):
                options = [f"{k}:{v}" for k, v in r['é€‰é¡¹'].items() if v]
                if options:
                    f.write(f"é€‰é¡¹: {' | '.join(options)}\n")
            f.write(f"æ–‡æœ¬å†…å®¹:\n{r.get('åŸå§‹æ–‡æœ¬', '')}\n")
            f.write("-" * 30 + "\n\n")
    
    end_time = time.time()
    total_time = end_time - start_time
    avg_time_per_image = total_time / len(files) if files else 0

    print("\n" + "=" * 60)
    print("OCR å¤„ç†å®Œæˆï¼")
    print(f"æˆåŠŸ: {len(results)} | å¤±è´¥: {len(errors)}")
    print(f"æ€»è€—æ—¶: {total_time:.2f}ç§’")
    print(f"å¹³å‡æ¯å¼ å›¾ç‰‡: {avg_time_per_image:.2f}ç§’")
    print(f"å¤„ç†é€Ÿåº¦: {len(results)/total_time:.2f} å¼ /ç§’" if total_time > 0 else "å¤„ç†é€Ÿåº¦: N/A")
    print(f"ä¸­é—´ç»“æœ: processing/{TIMESTAMP}/{TASK_ID}/ocr/")
    print(f"æœ€ç»ˆç»“æœ: output/{TIMESTAMP}/{TASK_ID}/")
    print("  - ocr_results.json")
    print("  - ocr_results.csv")
    print("  - ocr_summary.txt")
    if parsing_mode:
        print("  - reasoning.json")
    print("=" * 60)

    if errors:
        print("\nå¤±è´¥åˆ—è¡¨:")
        for e in errors:
            print(f"  - {e.get('æ–‡ä»¶', '?')}: {e.get('é”™è¯¯', '?')}")

    # GPUåŠ é€Ÿæ€§èƒ½æç¤º
    if use_gpu:
        print("\nğŸ’¡ GPUåŠ é€Ÿå·²å¯ç”¨ï¼Œæ€§èƒ½æ•°æ®å¦‚ä¸Šæ‰€ç¤º")
    else:
        print("\nğŸ’¡ å½“å‰ä½¿ç”¨CPUæ¨¡å¼ï¼Œå¦‚éœ€GPUåŠ é€Ÿè¯·å®‰è£…CUDAå’Œç›¸åº”ä¾èµ–")
        print("   å®‰è£…æŒ‡å—: https://onnxruntime.ai/docs/execution-providers/CUDA-ExecutionProvider.html")

if __name__ == "__main__":
    main()
